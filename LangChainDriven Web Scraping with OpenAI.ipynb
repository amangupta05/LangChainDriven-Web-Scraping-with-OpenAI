{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "C:\\Users\\amang\\AppData\\Local\\Temp\\ipykernel_10224\\2258370248.py:17: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  openai = OpenAI(api_key=OPENAI_API_KEY)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Text:\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Artificial intelligence - Wikipedia\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jump to content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\t\tNavigation\n",
      "\t\n",
      "\n",
      "\n",
      "Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tContribute\n",
      "\t\n",
      "\n",
      "\n",
      "HelpLearn to editCommunity portalRecent changesUpload file\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Donate\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Appearance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Create account\n",
      "\n",
      "Log in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Personal tools\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Create account Log in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tPages for logged out editors learn more\n",
      "\n",
      "\n",
      "\n",
      "ContributionsTalk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Top)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "Goals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle Goals subsection\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.1\n",
      "Reasoning and problem-solving\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.2\n",
      "Knowledge representation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.3\n",
      "Planning and decision-making\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.4\n",
      "Learning\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.5\n",
      "Natural language processing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.6\n",
      "Perception\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.7\n",
      "Social intelligence\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.8\n",
      "General intelligence\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "Techniques\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle Techniques subsection\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.1\n",
      "Search and\n",
      "Number of chunks created: 389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amang\\miniconda3\\envs\\langchainEnv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\amang\\miniconda3\\envs\\langchainEnv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors created: 389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amang\\AppData\\Local\\Temp\\ipykernel_10224\\2258370248.py:53: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/v0.2/docs/versions/migrating_chains/retrieval_qa/\n",
      "  retrieval_qa = RetrievalQA(vector_store=vector_store, llm=openai)\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "4 validation errors for RetrievalQA\ncombine_documents_chain\n  Field required [type=missing, input_value={'vector_store': <langcha...roxy='', logit_bias={})}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nretriever\n  Field required [type=missing, input_value={'vector_store': <langcha...roxy='', logit_bias={})}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nvector_store\n  Extra inputs are not permitted [type=extra_forbidden, input_value=<langchain_community.vect...t at 0x000001CB8B1383D0>, input_type=FAISS]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nllm\n  Extra inputs are not permitted [type=extra_forbidden, input_value=OpenAI(client=<openai.res...proxy='', logit_bias={}), input_type=OpenAI]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m store_vectors(chunks, embeddings)  \u001b[38;5;66;03m# Pass embeddings for FAISS\u001b[39;00m\n\u001b[0;32m     78\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your query: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m retrieval_qa \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_retrieval_qa_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m response \u001b[38;5;241m=\u001b[39m get_llm_response(query, retrieval_qa)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "Cell \u001b[1;32mIn[1], line 53\u001b[0m, in \u001b[0;36mcreate_retrieval_qa_chain\u001b[1;34m(vector_store)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_retrieval_qa_chain\u001b[39m(vector_store):\n\u001b[1;32m---> 53\u001b[0m     retrieval_qa \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenai\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retrieval_qa\n",
      "File \u001b[1;32mc:\\Users\\amang\\miniconda3\\envs\\langchainEnv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     emit_warning()\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amang\\miniconda3\\envs\\langchainEnv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     emit_warning()\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amang\\miniconda3\\envs\\langchainEnv\\lib\\site-packages\\langchain_core\\load\\serializable.py:110\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amang\\miniconda3\\envs\\langchainEnv\\lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 4 validation errors for RetrievalQA\ncombine_documents_chain\n  Field required [type=missing, input_value={'vector_store': <langcha...roxy='', logit_bias={})}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nretriever\n  Field required [type=missing, input_value={'vector_store': <langcha...roxy='', logit_bias={})}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nvector_store\n  Extra inputs are not permitted [type=extra_forbidden, input_value=<langchain_community.vect...t at 0x000001CB8B1383D0>, input_type=FAISS]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nllm\n  Extra inputs are not permitted [type=extra_forbidden, input_value=OpenAI(client=<openai.res...proxy='', logit_bias={}), input_type=OpenAI]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain import OpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Updated import\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Initialize OpenAI\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Function to scrape the website\n",
    "def scrape_website(url):\n",
    "    try:\n",
    "        loader = WebBaseLoader(url)\n",
    "        documents = loader.load()\n",
    "        if documents:\n",
    "            content = \" \".join([doc.page_content for doc in documents])\n",
    "            return content\n",
    "        else:\n",
    "            print(\"No documents found.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping the website: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to divide text into chunks\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Function to convert text chunks into vector embeddings\n",
    "def text_to_vectors(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectors = embeddings.embed_documents(chunks)  # Use embed_documents method\n",
    "    return vectors, embeddings  # Return embeddings too\n",
    "\n",
    "# Function to store vectors in a vector store database\n",
    "def store_vectors(chunks, embeddings):\n",
    "    # Create a FAISS index using from_texts\n",
    "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "# Function to create a retrieval QA chain\n",
    "def create_retrieval_qa_chain(vector_store):\n",
    "    # Ensure that you are passing the correct arguments to RetrievalQA\n",
    "    retrieval_qa = RetrievalQA.from_chain_type(\n",
    "        llm=openai, \n",
    "        chain_type=\"stuff\",  # Or \"map_reduce\", depending on your needs\n",
    "        retriever=vector_store.as_retriever()\n",
    "    )\n",
    "    return retrieval_qa\n",
    "\n",
    "# Function to get response from LLM\n",
    "def get_llm_response(query, retrieval_qa):\n",
    "    response = retrieval_qa.run(query)\n",
    "    return response\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter the website URL to scrape: \")  # Ask the user for the website URL\n",
    "    scraped_text = scrape_website(url)\n",
    "\n",
    "    if scraped_text is None or scraped_text.strip() == \"\":\n",
    "        print(\"Failed to retrieve the website content.\")\n",
    "    else:\n",
    "        print(\"Scraped Text:\\n\", scraped_text[:1000])  # Print the first 1000 characters of the scraped text\n",
    "        \n",
    "        chunks = chunk_text(scraped_text)\n",
    "        print(f\"Number of chunks created: {len(chunks)}\")  # Debugging print\n",
    "        vectors, embeddings = text_to_vectors(chunks)  # Get vectors and embeddings\n",
    "        print(f\"Number of vectors created: {len(vectors)}\")  # Debugging print\n",
    "        \n",
    "        vector_store = store_vectors(chunks, embeddings)  # Pass embeddings for FAISS\n",
    "        \n",
    "        query = input(\"Enter your query: \")\n",
    "        retrieval_qa = create_retrieval_qa_chain(vector_store)\n",
    "        \n",
    "        response = get_llm_response(query, retrieval_qa)\n",
    "        print(\"Response:\\n\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchainEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
